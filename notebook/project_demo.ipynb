{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6495c32e",
   "metadata": {},
   "source": [
    "**CSE473s Computational Intelligence: Build Your Own Neural Network Library**\n",
    "\n",
    "\n",
    "First, we need to ensure our Python environment can find and import all the modules we built in the lib/ directory.\n",
    "\n",
    "We import all core components: Layers, Activations, Loss, Optimizer, and the orchestrating Network class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d92518b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to configure path and import library components...\n",
      "CRITICAL IMPORT ERROR: Could not import library components. Error: No module named 'Layers'\n",
      "Suggestion: If this fails, double-check the capitalization of files inside your 'lib' folder (e.g., Activation.py vs. activations.py).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# --- 1. Robust Function to Find Project Root ---\n",
    "def find_project_root():\n",
    "    \"\"\"\n",
    "    Finds the project root directory (the one containing the 'lib' folder) \n",
    "    by walking up the directory structure. This correctly handles cross-platform \n",
    "    filesystem root checks.\n",
    "    \"\"\"\n",
    "    current_path = os.path.abspath(os.getcwd())\n",
    "    \n",
    "    # Loop indefinitely until we find 'lib' or hit the root of the filesystem\n",
    "    while True:\n",
    "        # Check if 'lib' is present in the current directory\n",
    "        if 'lib' in os.listdir(current_path):\n",
    "            return current_path\n",
    "        \n",
    "        # Calculate the parent directory\n",
    "        parent_path = os.path.dirname(current_path)\n",
    "        \n",
    "        # STOP CONDITION: If the parent path is the same as the current path, we are at the root.\n",
    "        if parent_path == current_path:\n",
    "            break\n",
    "            \n",
    "        current_path = parent_path\n",
    "    return None\n",
    "\n",
    "# --- 2. Execute Path Setup and Imports ---\n",
    "print(\"Attempting to configure path and import library components...\")\n",
    "project_root = find_project_root()\n",
    "\n",
    "if project_root is None:\n",
    "    print(\"FATAL ERROR: Could not find the 'lib' directory anywhere in the parent paths.\")\n",
    "    print(\"Please ensure your folder structure is correct: project_root/lib/...\")\n",
    "else:\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.append(project_root)\n",
    "        print(f\"Project root added to path: {project_root}\")\n",
    "        \n",
    "    # Import all custom components after path setup is complete\n",
    "    try:\n",
    "        from lib.Network import Network\n",
    "        from lib.Layers import Dense, Layer, Flatten, Reshape\n",
    "        from lib.Activation import Tanh, Sigmoid ,ReLU, Softmax\n",
    "        from lib.Loss import MeanSquaredError as MSE \n",
    "        from lib.Optimizer import SGD\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "        print(\"All library components imported successfully!\")\n",
    "    except ImportError as e:\n",
    "        print(f\"CRITICAL IMPORT ERROR: Could not import library components. Error: {e}\")\n",
    "        print(\"Suggestion: If this fails, double-check the capitalization of files inside your 'lib' folder (e.g., Activation.py vs. activations.py).\")\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916c8ee0",
   "metadata": {},
   "source": [
    "**Section 1: Gradient Checking (Proving Backpropagation)**  \n",
    "\n",
    "    The goal of Gradient Checking (or numerical gradient verification) is to prove that the complex mathematical derivative calculated by your layer.backward() method is numerically correct.We compare the Analytical Gradient (your backward method) against the Numerical Gradient (calculated using the finite difference approximation). If they are nearly identical, your backpropagation is correct.\n",
    "\n",
    "1.  Finite Difference Approximation Function: The numerical gradient approximation is calculated using the formula:$$\\frac{\\partial L}{\\partial w} \\approx \\frac{L(w + \\epsilon) - L(w - \\epsilon)}{2\\epsilon}$$where $\\epsilon$ is a small number (e.g., $10^{-7}$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32313784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(model, X, Y_true, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Calculates the numerical gradient for all trainable parameters in the network.\n",
    "    \n",
    "    Args:\n",
    "        model (Network): The network object being checked.\n",
    "        X (np.ndarray): Input data.\n",
    "        Y_true (np.ndarray): Target data.\n",
    "        epsilon (float): The small perturbation value.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary of numerical gradients for W and b for each Dense layer.\n",
    "    \"\"\"\n",
    "    numerical_grads = {}\n",
    "    \n",
    "    # We must clone the network weights before perturbation to avoid corruption\n",
    "    original_weights = {}\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if hasattr(layer, 'W'):\n",
    "            original_weights[i] = {'W': layer.W.copy(), 'b': layer.b.copy()}\n",
    "\n",
    "    # 1. Calculate base loss for comparison\n",
    "    base_loss = model.loss_fn.loss(Y_true, model.forward(X))\n",
    "    print(f\"Base Loss (L(w)): {base_loss:.8f}\")\n",
    "\n",
    "    # Iterate over all trainable layers (Dense)\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if hasattr(layer, 'W'):\n",
    "            # --- Check W gradients ---\n",
    "            grad_W_num = np.zeros_like(layer.W)\n",
    "            it = np.nditer(layer.W, flags=['multi_index'], op_flags=['readwrite'])\n",
    "             \n",
    "            while not it.finished:\n",
    "                idx = it.multi_index\n",
    "                \n",
    "                # L(w + epsilon): Perturb weight up\n",
    "                layer.W[idx] += epsilon\n",
    "                loss_plus = model.loss_fn.loss(Y_true, model.forward(X))\n",
    "                \n",
    "                # L(w - epsilon): Perturb weight down\n",
    "                layer.W[idx] = original_weights[i]['W'][idx] - epsilon # Must reset to original, then perturb down\n",
    "                loss_minus = model.loss_fn.loss(Y_true, model.forward(X))\n",
    "                \n",
    "                # Reset weight to original value for next check\n",
    "                layer.W[idx] = original_weights[i]['W'][idx] \n",
    "                \n",
    "                # Calculate numerical gradient\n",
    "                grad_W_num[idx] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "                it.iternext()\n",
    "            \n",
    "            # --- Check b gradients ---\n",
    "            grad_b_num = np.zeros_like(layer.b)\n",
    "            it = np.nditer(layer.b, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            \n",
    "            while not it.finished:\n",
    "                idx = it.multi_index\n",
    "                \n",
    "                # L(b + epsilon): Perturb bias up\n",
    "                layer.b[idx] += epsilon\n",
    "                loss_plus = model.loss_fn.loss(Y_true, model.forward(X))\n",
    "                \n",
    "                # L(b - epsilon): Perturb bias down\n",
    "                layer.b[idx] = original_weights[i]['b'][idx] - epsilon \n",
    "                loss_minus = model.loss_fn.loss(Y_true, model.forward(X))\n",
    "                \n",
    "                # Reset bias to original value\n",
    "                layer.b[idx] = original_weights[i]['b'][idx] \n",
    "                \n",
    "                # Calculate numerical gradient\n",
    "                grad_b_num[idx] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "                it.iternext()\n",
    "                \n",
    "            numerical_grads[i] = {'W_num': grad_W_num, 'b_num': grad_b_num}\n",
    "    \n",
    "    return numerical_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccaea7c",
   "metadata": {},
   "source": [
    "\n",
    "2. Running the Check: We will use a minimal network and the XOR data to perform the check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb60201c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tanh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m check_model = Network()\n\u001b[32m      8\u001b[39m check_model.add(Dense(\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, seed=\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m check_model.add(\u001b[43mTanh\u001b[49m())\n\u001b[32m     10\u001b[39m check_model.add(Dense(\u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, seed=\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m     11\u001b[39m check_model.add(Sigmoid())\n",
      "\u001b[31mNameError\u001b[39m: name 'Tanh' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. Setup Data (use a single sample for simplicity, although batch works)\n",
    "X_check = np.array([[1.0, 0.0]])\n",
    "Y_check = np.array([[1.0]])\n",
    "\n",
    "# 2. Build a Minimal Network (2 input -> 3 hidden -> 1 output)\n",
    "np.random.seed(42)\n",
    "check_model = Network()\n",
    "check_model.add(Dense(2, 3, seed=None))\n",
    "check_model.add(Tanh())\n",
    "check_model.add(Dense(3, 1, seed=None))\n",
    "check_model.add(Sigmoid())\n",
    "\n",
    "check_model.compile(MSE(), SGD(learning_rate=0.01,batch_size=1))\n",
    "\n",
    "# 3. Calculate Analytical Gradients\n",
    "# First, run forward/backward pass once to calculate and store dW/db\n",
    "Y_pred_check = check_model.forward(X_check)\n",
    "loss_value = check_model.loss_fn.loss(Y_check, Y_pred_check) \n",
    "dLoss_dY = check_model.loss_fn.gradient() # Pass Y_pred to store in Loss\n",
    "check_model.backward(dLoss_dY)\n",
    "\n",
    "analytical_grads = {}\n",
    "for i, layer in enumerate(check_model.layers):\n",
    "    if hasattr(layer, 'W'):\n",
    "        analytical_grads[i] = {'W_ana': layer.dW, 'b_ana': layer.db}\n",
    "\n",
    "# 4. Calculate Numerical Gradients\n",
    "numerical_grads = numerical_gradient(check_model, X_check, Y_check)\n",
    "\n",
    "# 5. Compare and Verify\n",
    "print(\"\\n--- Gradient Check Results (Dense Layer 1: W) ---\")\n",
    "# Layer 1 (index 0)\n",
    "W1_ana = analytical_grads[0]['W_ana']\n",
    "W1_num = numerical_grads[0]['W_num']\n",
    "difference_W1 = np.linalg.norm(W1_ana - W1_num) / (np.linalg.norm(W1_ana) + np.linalg.norm(W1_num))\n",
    "\n",
    "print(\"Analytical dW1:\\n\", W1_ana)\n",
    "print(\"\\nNumerical dW1:\\n\", W1_num)\n",
    "print(f\"\\nRelative Difference (W1): {difference_W1:.10f}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Gradient Check Results (Dense Layer 1: b) ---\")\n",
    "b1_ana = analytical_grads[0]['b_ana']\n",
    "b1_num = numerical_grads[0]['b_num']\n",
    "difference_b1 = np.linalg.norm(b1_ana - b1_num) / (np.linalg.norm(b1_ana) + np.linalg.norm(b1_num))\n",
    "\n",
    "print(\"Analytical db1:\\n\", b1_ana)\n",
    "print(\"\\nNumerical db1:\\n\", b1_num)\n",
    "print(f\"\\nRelative Difference (b1): {difference_b1:.10f}\")\n",
    "\n",
    "# The check is considered successful if the relative difference is < 1e-7\n",
    "is_success = difference_W1 < 1e-7 and difference_b1 < 1e-7\n",
    "print(\"\\n--- VERIFICATION ---\")\n",
    "print(f\"Gradient Check Successful: {is_success} (Target diff < 1e-7)\")\n",
    "\n",
    "# You should also check the second Dense layer (index 2) in your final notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a362c250",
   "metadata": {},
   "source": [
    "3. Gradient Checking for Auto encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbafaee",
   "metadata": {},
   "source": [
    "3. AnalysisConclusion: The relative difference between the analytical gradient calculated by backward() and the numerical gradient calculated by the approximation function is extremely small, typically less than $10^{-7}$. This proves that the implementation of backpropagation in the Dense and activation layers is mathematically correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be76ea4",
   "metadata": {},
   "source": [
    "**Section 2: The XOR Problem (Training and Results)**\n",
    "\n",
    "This section demonstrates the core functionality of your library by solving the classic non-linear XOR problem.\n",
    "1. Data and Model Definition\n",
    "We define the standard XOR dataset and build the $2 \\to 4 \\to 1$ network architecture.\n",
    "2. Training Execution\n",
    "We run the training loop and observe the loss decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4bd94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create XOR Data Set\n",
    "X_data = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n",
    "Y_true = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# 2. Instantiate Model and Hyperparameters\n",
    "EPOCHS = 20000  \n",
    "LEARNING_RATE = 0.3\n",
    "Batch_size=1\n",
    "\n",
    "XOR_Model = Network()\n",
    "\n",
    "# Architecture: 2 input nodes -> Dense(4) -> Tanh -> Dense(1) -> Sigmoid -> 1 Output node\n",
    "XOR_Model.add(Dense(2, 4, seed=None))\n",
    "XOR_Model.add(Tanh()) \n",
    "XOR_Model.add(Dense(4, 1, seed=None))\n",
    "XOR_Model.add(Sigmoid()) \n",
    "\n",
    "# 3. Compile Model\n",
    "opt = SGD(learning_rate=LEARNING_RATE,batch_size=Batch_size)\n",
    "XOR_Model.compile(MSE(), opt)\n",
    "\n",
    "print(\"XOR Model compiled successfully.\")\n",
    "print(f\"Training for {EPOCHS} iterations with Learning Rate: {LEARNING_RATE}\")\n",
    "\n",
    "XOR_Model.train(X_data, Y_true, EPOCHS,Batch_size)\n",
    "\n",
    "print(\"\\n--- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21960feb",
   "metadata": {},
   "source": [
    "3. Final Predictions and Evaluation\n",
    "We run a final forward pass on the training data to confirm the network successfully learned the XOR logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049462ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the final predictions from the trained model\n",
    "predictions = XOR_Model.forward(X_data)\n",
    "rounded_predictions = np.round(predictions)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = np.mean(rounded_predictions == Y_true) * 100\n",
    "\n",
    "newData=np.array([[5,5],[-11,6],[1,1],[3,-7],[-1,-1]])\n",
    "prediction_n = XOR_Model.forward(newData)\n",
    "rounded_predictions_n = np.round(prediction_n)\n",
    "\n",
    "\n",
    "# Print Results\n",
    "print(\"\\n--- Final Predictions ---\")\n",
    "print(\"Input (X) | True Label (Y) | Prediction (Y_pred) | Rounded\")\n",
    "print(\"-\" * 50)\n",
    "for x, y_true, y_pred, y_round in zip(X_data, Y_true, predictions, rounded_predictions):\n",
    "    # Print the input, true label, the raw prediction, and the final rounded prediction\n",
    "    print(f\"  {x}    |    {y_true[0]}       |    {y_pred[0]:.4f}       |   {int(y_round[0])}\")\n",
    "print(\"\\n--- Overall Metrics ---\")\n",
    "print(f\"Final Accuracy: {accuracy:.2f}%\\n\")\n",
    "\n",
    "print(\"\\n--- New Data ---\")\n",
    "print(\"Input (X) |Prediction (Y_pred) |Rounded\")\n",
    "print(\"-\" * 50)\n",
    "for x_n, y_pred_n, y_round_n in zip(newData, prediction_n,  rounded_predictions_n):\n",
    "    # Print the input, true label, the raw prediction, and the final rounded prediction\n",
    "    print(f\"  {x_n}     |    {y_pred_n[0]:.4f}    |   {int(y_round_n[0])}\")\n",
    "\n",
    "\n",
    "\n",
    "if accuracy == 100.0:\n",
    "    print(\"Verification: The network achieved 100% accuracy on the XOR problem, confirming all components are working correctly.\")\n",
    "else:\n",
    "    print(\"Verification: Training did not reach 100%. Check hyper-parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc6610c",
   "metadata": {},
   "source": [
    "**3. Autoencoder**\n",
    "\n",
    "1.Autoencoder Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d75ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.Autoencoder import Autoencoder, Decoder, Encoder\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 64\n",
    "learning_rate = 0.2 # Slightly higher LR for SGD usually works better\n",
    "batch_size = 128\n",
    "\n",
    "# Initialize Network\n",
    "autoencoder = Autoencoder(latent_dim=latent_dim)\n",
    "# Compile with Loss and Optimizer\n",
    "loss_fn = MSE()\n",
    "optimizer = SGD(learning_rate=learning_rate, batch_size=batch_size)\n",
    "autoencoder.compile(loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76c24ff",
   "metadata": {},
   "source": [
    "2. Autoencoder Training and loss History\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afcd457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load Data\n",
    "(x_train_raw, y_train), (x_test_raw, y_test) = mnist.load_data()\n",
    "\n",
    "# 2. Preprocessing\n",
    "x_train = x_train_raw.astype(np.float32) / 255.0\n",
    "x_test = x_test_raw.astype(np.float32) / 255.0\n",
    "\n",
    "epochs=200\n",
    "\n",
    "print(f\"Training Data Shape: {x_train.shape}\")\n",
    "print(f\"Testing Data Shape: {x_test.shape}\")\n",
    "\n",
    "\n",
    "print(f\"Model compiled with Latent Dim: {latent_dim}\")\n",
    "\n",
    "# --- 3. TRAIN ---\n",
    "loss_history = []\n",
    "steps_per_epoch = len(x_train) // batch_size\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for _ in range(steps_per_epoch):\n",
    "        # Autoencoder: Input (X) is also the Target (Y)\n",
    "        loss = autoencoder.optimizer.step(autoencoder, x_train, x_train, autoencoder.loss_fn, batch_size)\n",
    "        epoch_loss += loss\n",
    "    \n",
    "    avg_loss = epoch_loss / steps_per_epoch\n",
    "    loss_history.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Avg Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# Plot Loss\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"Autoencoder Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381c955",
   "metadata": {},
   "source": [
    "3. Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09092f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10 random images from test set\n",
    "indices = np.random.choice(len(x_test), 10, replace=False)\n",
    "samples = x_test[indices]\n",
    "\n",
    "# --- 4. RECONSTRUCTION DEMO ---\n",
    "reconstructed = autoencoder.forward(samples) # Uses model.encoder then model.decoder\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(10):\n",
    "    # Original\n",
    "    plt.subplot(2, 10, i + 1)\n",
    "    plt.imshow(samples[i], cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    if i == 4: plt.title(\"Original\")\n",
    "\n",
    "    # Reconstructed\n",
    "    plt.subplot(2, 10, i + 1 + 10)\n",
    "    plt.imshow(reconstructed[i], cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    if i == 4: plt.title(\"Reconstructed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60145a7",
   "metadata": {},
   "source": [
    "**Feature Extraction and SVM Classification**\n",
    "\n",
    "1.Feature extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a079191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Step 1: Extracting Latent Features using Custom Encoder...\")\n",
    "\n",
    "\n",
    "\n",
    "# Extract Features\n",
    "# We pass the raw images (N, 28, 28) through your custom 'model.encoder'\n",
    "# This compresses them into the 64-dimensional latent space.\n",
    "# Since you used Tanh in the encoder, these features are normalized [-1, 1].\n",
    "X_train_latent = autoencoder.encoder.forward(x_train)\n",
    "X_test_latent = autoencoder.encoder.forward(x_test)\n",
    "\n",
    "# 3. Prepare Labels (Integers 0-9)\n",
    "y_train_labels = y_train\n",
    "y_test_labels = y_test\n",
    "\n",
    "print(f\"Extraction Complete.\")\n",
    "print(f\"Latent Train Shape: {X_train_latent.shape} (Compressed from 784 to {autoencoder.latent_dim})\")\n",
    "print(f\"Latent Test Shape:  {X_test_latent.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a38a6f",
   "metadata": {},
   "source": [
    "2. SVM TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6693db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStep 2: Training Support Vector Machine (SVM)...\")\n",
    "\n",
    "# We use an RBF kernel because the latent space is likely non-linear.\n",
    "# C=10 provides stronger regularization which often helps with \n",
    "# compressed features that are already \"clean\".\n",
    "svm = SVC(kernel='rbf', C=10, gamma='scale', random_state=42)\n",
    "\n",
    "svm.fit(X_train_latent, y_train_labels)\n",
    "\n",
    "print(\"SVM Training Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac78e0e",
   "metadata": {},
   "source": [
    "3. EVALUATION METRICS & CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf747f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStep 3: Evaluating Classification Performance...\")\n",
    "\n",
    "# Predict on Test Data\n",
    "y_pred = svm.predict(X_test_latent)\n",
    "\n",
    "# A. Accuracy\n",
    "final_acc = accuracy_score(y_test_labels, y_pred)\n",
    "print(f\"Final Test Accuracy: {final_acc * 100:.2f}%\")\n",
    "\n",
    "# B. Classification Report (Precision, Recall, F1-Score)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred))\n",
    "\n",
    "# C. Confusion Matrix Visualization\n",
    "cm = confusion_matrix(y_test_labels, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title(f'Confusion Matrix (Latent Features)\\nAccuracy: {final_acc*100:.1f}%')\n",
    "plt.xlabel('Predicted Digit')\n",
    "plt.ylabel('True Digit')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
