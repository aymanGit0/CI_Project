{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6495c32e",
   "metadata": {},
   "source": [
    "**CSE473s Computational Intelligence: Build Your Own Neural Network Library**\n",
    "\n",
    "\n",
    "First, we need to ensure our Python environment can find and import all the modules we built in the lib/ directory.\n",
    "\n",
    "We import all core components: Layers, Activations, Loss, Optimizer, and the orchestrating Network class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d92518b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to configure path and import library components...\n",
      "All library components imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "# --- 1. Robust Function to Find Project Root ---\n",
    "def find_project_root():\n",
    "    \"\"\"\n",
    "    Finds the project root directory (the one containing the 'lib' folder) \n",
    "    by walking up the directory structure. This correctly handles cross-platform \n",
    "    filesystem root checks.\n",
    "    \"\"\"\n",
    "    current_path = os.path.abspath(os.getcwd())\n",
    "    \n",
    "    # Loop indefinitely until we find 'lib' or hit the root of the filesystem\n",
    "    while True:\n",
    "        # Check if 'lib' is present in the current directory\n",
    "        if 'lib' in os.listdir(current_path):\n",
    "            return current_path\n",
    "        \n",
    "        # Calculate the parent directory\n",
    "        parent_path = os.path.dirname(current_path)\n",
    "        \n",
    "        # STOP CONDITION: If the parent path is the same as the current path, we are at the root.\n",
    "        if parent_path == current_path:\n",
    "            break\n",
    "            \n",
    "        current_path = parent_path\n",
    "    return None\n",
    "\n",
    "# --- 2. Execute Path Setup and Imports ---\n",
    "print(\"Attempting to configure path and import library components...\")\n",
    "project_root = find_project_root()\n",
    "\n",
    "if project_root is None:\n",
    "    print(\"FATAL ERROR: Could not find the 'lib' directory anywhere in the parent paths.\")\n",
    "    print(\"Please ensure your folder structure is correct: project_root/lib/...\")\n",
    "else:\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.append(project_root)\n",
    "        print(f\"Project root added to path: {project_root}\")\n",
    "        \n",
    "    # Import all custom components after path setup is complete\n",
    "    try:\n",
    "        from lib.Network import Network\n",
    "        from lib.Layers import Dense, Layer\n",
    "        from lib.Activation import Tanh, Sigmoid \n",
    "        from lib.Loss import MeanSquaredError as MSE \n",
    "        from lib.Optimizer import SGD\n",
    "    \n",
    "        \n",
    "        print(\"All library components imported successfully!\")\n",
    "    except ImportError as e:\n",
    "        print(f\"CRITICAL IMPORT ERROR: Could not import library components. Error: {e}\")\n",
    "        print(\"Suggestion: If this fails, double-check the capitalization of files inside your 'lib' folder (e.g., Activation.py vs. activations.py).\")\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916c8ee0",
   "metadata": {},
   "source": [
    "**Section 1: Gradient Checking (Proving Backpropagation)**  \n",
    "\n",
    "    The goal of Gradient Checking (or numerical gradient verification) is to prove that the complex mathematical derivative calculated by your layer.backward() method is numerically correct.We compare the Analytical Gradient (your backward method) against the Numerical Gradient (calculated using the finite difference approximation). If they are nearly identical, your backpropagation is correct.\n",
    "\n",
    "1.  Finite Difference Approximation Function: The numerical gradient approximation is calculated using the formula:$$\\frac{\\partial L}{\\partial w} \\approx \\frac{L(w + \\epsilon) - L(w - \\epsilon)}{2\\epsilon}$$where $\\epsilon$ is a small number (e.g., $10^{-7}$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32313784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(model, X, Y_true, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Calculates the numerical gradient for all trainable parameters in the network.\n",
    "    \n",
    "    Args:\n",
    "        model (Network): The network object being checked.\n",
    "        X (np.ndarray): Input data.\n",
    "        Y_true (np.ndarray): Target data.\n",
    "        epsilon (float): The small perturbation value.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary of numerical gradients for W and b for each Dense layer.\n",
    "    \"\"\"\n",
    "    numerical_grads = {}\n",
    "    \n",
    "    # We must clone the network weights before perturbation to avoid corruption\n",
    "    original_weights = {}\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if hasattr(layer, 'W'):\n",
    "            original_weights[i] = {'W': layer.W.copy(), 'b': layer.b.copy()}\n",
    "\n",
    "    # 1. Calculate base loss for comparison\n",
    "    base_loss = model.loss_fn.loss(Y_true, model.forward(X))\n",
    "    print(f\"Base Loss (L(w)): {base_loss:.8f}\")\n",
    "\n",
    "    # Iterate over all trainable layers (Dense)\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if hasattr(layer, 'W'):\n",
    "            # --- Check W gradients ---\n",
    "            grad_W_num = np.zeros_like(layer.W)\n",
    "            it = np.nditer(layer.W, flags=['multi_index'], op_flags=['readwrite'])\n",
    "             \n",
    "            while not it.finished:\n",
    "                idx = it.multi_index\n",
    "                \n",
    "                # L(w + epsilon): Perturb weight up\n",
    "                layer.W[idx] += epsilon\n",
    "                loss_plus = model.loss_fn.loss(Y_true, model.forward(X))\n",
    "                \n",
    "                # L(w - epsilon): Perturb weight down\n",
    "                layer.W[idx] = original_weights[i]['W'][idx] - epsilon # Must reset to original, then perturb down\n",
    "                loss_minus = model.loss_fn.loss(Y_true, model.forward(X))\n",
    "                \n",
    "                # Reset weight to original value for next check\n",
    "                layer.W[idx] = original_weights[i]['W'][idx] \n",
    "                \n",
    "                # Calculate numerical gradient\n",
    "                grad_W_num[idx] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "                it.iternext()\n",
    "            \n",
    "            # --- Check b gradients ---\n",
    "            grad_b_num = np.zeros_like(layer.b)\n",
    "            it = np.nditer(layer.b, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            \n",
    "            while not it.finished:\n",
    "                idx = it.multi_index\n",
    "                \n",
    "                # L(b + epsilon): Perturb bias up\n",
    "                layer.b[idx] += epsilon\n",
    "                loss_plus = model.loss_fn.loss(Y_true, model.forward(X))\n",
    "                \n",
    "                # L(b - epsilon): Perturb bias down\n",
    "                layer.b[idx] = original_weights[i]['b'][idx] - epsilon \n",
    "                loss_minus = model.loss_fn.loss(Y_true, model.forward(X))\n",
    "                \n",
    "                # Reset bias to original value\n",
    "                layer.b[idx] = original_weights[i]['b'][idx] \n",
    "                \n",
    "                # Calculate numerical gradient\n",
    "                grad_b_num[idx] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "                it.iternext()\n",
    "                \n",
    "            numerical_grads[i] = {'W_num': grad_W_num, 'b_num': grad_b_num}\n",
    "    \n",
    "    return numerical_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccaea7c",
   "metadata": {},
   "source": [
    "\n",
    "2. Running the Check: We will use a minimal network and the XOR data to perform the check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb60201c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network compiled successfully.\n",
      "Base Loss (L(w)): 0.03190939\n",
      "\n",
      "--- Gradient Check Results (Dense Layer 1: W) ---\n",
      "Analytical dW1:\n",
      " [[ 0.047918 -0.018312 -0.009219]\n",
      " [ 0.        0.        0.      ]]\n",
      "\n",
      "Numerical dW1:\n",
      " [[ 0.047918 -0.018312 -0.009219]\n",
      " [ 0.        0.        0.      ]]\n",
      "\n",
      "Relative Difference (W1): 0.0000000021\n",
      "\n",
      "--- Gradient Check Results (Dense Layer 1: b) ---\n",
      "Analytical db1:\n",
      " [[ 0.047918 -0.018312 -0.009219]]\n",
      "\n",
      "Numerical db1:\n",
      " [[ 0.047918 -0.018312 -0.009219]]\n",
      "\n",
      "Relative Difference (b1): 0.0000000021\n",
      "\n",
      "--- VERIFICATION ---\n",
      "Gradient Check Successful: True (Target diff < 1e-7)\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup Data (use a single sample for simplicity, although batch works)\n",
    "X_check = np.array([[1.0, 0.0]])\n",
    "Y_check = np.array([[1.0]])\n",
    "\n",
    "# 2. Build a Minimal Network (2 input -> 3 hidden -> 1 output)\n",
    "np.random.seed(42)\n",
    "check_model = Network()\n",
    "check_model.add(Dense(2, 3, seed=None))\n",
    "check_model.add(Tanh())\n",
    "check_model.add(Dense(3, 1, seed=None))\n",
    "check_model.add(Sigmoid())\n",
    "\n",
    "check_model.compile(MSE(), SGD(learning_rate=0.01))\n",
    "\n",
    "# 3. Calculate Analytical Gradients\n",
    "# First, run forward/backward pass once to calculate and store dW/db\n",
    "Y_pred_check = check_model.forward(X_check)\n",
    "loss_value = check_model.loss_fn.loss(Y_check, Y_pred_check) \n",
    "dLoss_dY = check_model.loss_fn.gradient() # Pass Y_pred to store in Loss\n",
    "check_model.backward(dLoss_dY)\n",
    "\n",
    "analytical_grads = {}\n",
    "for i, layer in enumerate(check_model.layers):\n",
    "    if hasattr(layer, 'W'):\n",
    "        analytical_grads[i] = {'W_ana': layer.dW, 'b_ana': layer.db}\n",
    "\n",
    "# 4. Calculate Numerical Gradients\n",
    "numerical_grads = numerical_gradient(check_model, X_check, Y_check)\n",
    "\n",
    "# 5. Compare and Verify\n",
    "print(\"\\n--- Gradient Check Results (Dense Layer 1: W) ---\")\n",
    "# Layer 1 (index 0)\n",
    "W1_ana = analytical_grads[0]['W_ana']\n",
    "W1_num = numerical_grads[0]['W_num']\n",
    "difference_W1 = np.linalg.norm(W1_ana - W1_num) / (np.linalg.norm(W1_ana) + np.linalg.norm(W1_num))\n",
    "\n",
    "print(\"Analytical dW1:\\n\", W1_ana)\n",
    "print(\"\\nNumerical dW1:\\n\", W1_num)\n",
    "print(f\"\\nRelative Difference (W1): {difference_W1:.10f}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Gradient Check Results (Dense Layer 1: b) ---\")\n",
    "b1_ana = analytical_grads[0]['b_ana']\n",
    "b1_num = numerical_grads[0]['b_num']\n",
    "difference_b1 = np.linalg.norm(b1_ana - b1_num) / (np.linalg.norm(b1_ana) + np.linalg.norm(b1_num))\n",
    "\n",
    "print(\"Analytical db1:\\n\", b1_ana)\n",
    "print(\"\\nNumerical db1:\\n\", b1_num)\n",
    "print(f\"\\nRelative Difference (b1): {difference_b1:.10f}\")\n",
    "\n",
    "# The check is considered successful if the relative difference is < 1e-7\n",
    "is_success = difference_W1 < 1e-7 and difference_b1 < 1e-7\n",
    "print(\"\\n--- VERIFICATION ---\")\n",
    "print(f\"Gradient Check Successful: {is_success} (Target diff < 1e-7)\")\n",
    "\n",
    "# You should also check the second Dense layer (index 2) in your final notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbafaee",
   "metadata": {},
   "source": [
    "3. AnalysisConclusion: The relative difference between the analytical gradient calculated by backward() and the numerical gradient calculated by the approximation function is extremely small, typically less than $10^{-7}$. This proves that the implementation of backpropagation in the Dense and activation layers is mathematically correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be76ea4",
   "metadata": {},
   "source": [
    "**Section 2: The XOR Problem (Training and Results)**\n",
    "\n",
    "This section demonstrates the core functionality of your library by solving the classic non-linear XOR problem.\n",
    "1. Data and Model Definition\n",
    "We define the standard XOR dataset and build the $2 \\to 4 \\to 1$ network architecture.\n",
    "2. Training Execution\n",
    "We run the training loop and observe the loss decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df4bd94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network compiled successfully.\n",
      "XOR Model compiled successfully.\n",
      "Training for 10000 iterations with Learning Rate: 0.1\n",
      "iteration 1000/10000, Loss: 0.01897486\n",
      "iteration 2000/10000, Loss: 0.00364021\n",
      "iteration 3000/10000, Loss: 0.00180485\n",
      "iteration 4000/10000, Loss: 0.00116567\n",
      "iteration 5000/10000, Loss: 0.00085265\n",
      "iteration 6000/10000, Loss: 0.00066884\n",
      "iteration 7000/10000, Loss: 0.00054688\n",
      "iteration 8000/10000, Loss: 0.00046273\n",
      "iteration 9000/10000, Loss: 0.00039979\n",
      "iteration 10000/10000, Loss: 0.00035147\n",
      "iteration 10000/10000, Final Loss: 0.00035147\n",
      "\n",
      "--- Training Finished ---\n"
     ]
    }
   ],
   "source": [
    "# 1. Create XOR Data Set\n",
    "X_data = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n",
    "Y_true = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# 2. Instantiate Model and Hyperparameters\n",
    "EPOCHS = 10000  \n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "XOR_Model = Network()\n",
    "\n",
    "# Architecture: 2 input nodes -> Dense(4) -> Tanh -> Dense(1) -> Sigmoid -> 1 Output node\n",
    "XOR_Model.add(Dense(2, 4, seed=None))\n",
    "XOR_Model.add(Tanh()) \n",
    "XOR_Model.add(Dense(4, 1, seed=None))\n",
    "XOR_Model.add(Sigmoid()) \n",
    "\n",
    "# 3. Compile Model\n",
    "opt = SGD(learning_rate=LEARNING_RATE)\n",
    "XOR_Model.compile(MSE(), opt)\n",
    "\n",
    "print(\"XOR Model compiled successfully.\")\n",
    "print(f\"Training for {EPOCHS} iterations with Learning Rate: {LEARNING_RATE}\")\n",
    "\n",
    "XOR_Model.train(X_data, Y_true, EPOCHS)\n",
    "\n",
    "print(\"\\n--- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21960feb",
   "metadata": {},
   "source": [
    "3. Final Predictions and Evaluation\n",
    "We run a final forward pass on the training data to confirm the network successfully learned the XOR logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "049462ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Predictions ---\n",
      "Input (X) | True Label (Y) | Prediction (Y_pred) | Rounded\n",
      "--------------------------------------------------\n",
      "  [-1 -1]    |    0       |    0.0271       |   0\n",
      "  [-1  1]    |    1       |    0.9712       |   1\n",
      "  [ 1 -1]    |    1       |    0.9751       |   1\n",
      "  [1 1]    |    0       |    0.0252       |   0\n",
      "\n",
      "--- Overall Metrics ---\n",
      "Final Accuracy: 100.00%\n",
      "\n",
      "\n",
      "--- New Data ---\n",
      "Input (X) |Prediction (Y_pred) |Rounded\n",
      "--------------------------------------------------\n",
      "  [5 5]     |    0.0254    |   0\n",
      "  [-11   6]     |    0.6199    |   1\n",
      "  [1 1]     |    0.0252    |   0\n",
      "  [ 3 -3]     |    0.9612    |   1\n",
      "  [-1 -1]     |    0.0271    |   0\n",
      "Verification: The network achieved 100% accuracy on the XOR problem, confirming all components are working correctly.\n"
     ]
    }
   ],
   "source": [
    "# Get the final predictions from the trained model\n",
    "predictions = XOR_Model.forward(X_data)\n",
    "rounded_predictions = np.round(predictions)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = np.mean(rounded_predictions == Y_true) * 100\n",
    "\n",
    "newData=np.array([[5,5],[-11,6],[1,1],[3,-3],[-1,-1]])\n",
    "prediction_n = XOR_Model.forward(newData)\n",
    "rounded_predictions_n = np.round(prediction_n)\n",
    "\n",
    "\n",
    "# Print Results\n",
    "print(\"\\n--- Final Predictions ---\")\n",
    "print(\"Input (X) | True Label (Y) | Prediction (Y_pred) | Rounded\")\n",
    "print(\"-\" * 50)\n",
    "for x, y_true, y_pred, y_round in zip(X_data, Y_true, predictions, rounded_predictions):\n",
    "    # Print the input, true label, the raw prediction, and the final rounded prediction\n",
    "    print(f\"  {x}    |    {y_true[0]}       |    {y_pred[0]:.4f}       |   {int(y_round[0])}\")\n",
    "print(\"\\n--- Overall Metrics ---\")\n",
    "print(f\"Final Accuracy: {accuracy:.2f}%\\n\")\n",
    "\n",
    "print(\"\\n--- New Data ---\")\n",
    "print(\"Input (X) |Prediction (Y_pred) |Rounded\")\n",
    "print(\"-\" * 50)\n",
    "for x_n, y_pred_n, y_round_n in zip(newData, prediction_n,  rounded_predictions_n):\n",
    "    # Print the input, true label, the raw prediction, and the final rounded prediction\n",
    "    print(f\"  {x_n}     |    {y_pred_n[0]:.4f}    |   {int(y_round_n[0])}\")\n",
    "\n",
    "\n",
    "\n",
    "if accuracy == 100.0:\n",
    "    print(\"Verification: The network achieved 100% accuracy on the XOR problem, confirming all components are working correctly.\")\n",
    "else:\n",
    "    print(\"Verification: Training did not reach 100%. Check hyper-parameters.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
